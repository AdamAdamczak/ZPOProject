{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b67d8-1ee4-4637-9dc2-ed966fda9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning pytorch-metric-learning pillow albumentations timm\n",
    "!pip install neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c3470-f427-4874-9ca6-e9a3d73a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  !wget -O train.zip \"https://chmura.put.poznan.pl/s/Vc93ZH7TCtaWANk/download\"\n",
    "  !unzip -q train.zip\n",
    "\n",
    "  !wget -O test.zip \"https://chmura.put.poznan.pl/s/kJ6Q9tNpJ33osjr/download\"\n",
    "  !unzip -q test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07a741-9510-4fb6-b0fb-e22b42fa0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -q train.zip\n",
    "# !unzip -q test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5586e9-773d-4bc4-b4e2-a03cc6b8fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MetricLearningDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 places_dirs: list[Path],\n",
    "                 number_of_places_per_batch: int,\n",
    "                 number_of_images_per_place: int,\n",
    "                 number_of_batches_per_epoch: int,\n",
    "                 transforms: Callable):\n",
    "        super().__init__()\n",
    "\n",
    "        self._places_images_paths: list[list[Path]] = [\n",
    "            sorted([image_path for image_path in place_dir.iterdir() if image_path.is_file()])\n",
    "            for place_dir in places_dirs\n",
    "        ]\n",
    "        self._number_of_places = number_of_places_per_batch\n",
    "        self._number_of_images_per_place = number_of_images_per_place\n",
    "        self._number_of_samples_per_epoch = number_of_batches_per_epoch\n",
    "        self._transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._number_of_samples_per_epoch\n",
    "\n",
    "    def __getitem__(self, _: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        selected_places_indices = torch.randperm(len(self._places_images_paths))[:self._number_of_places]\n",
    "        transformed_images = []\n",
    "        selected_images_place_ids = []\n",
    "        for place_index in selected_places_indices:\n",
    "            place_images = self._places_images_paths[place_index]\n",
    "            selected_image_indices = torch.randperm(len(place_images))[:self._number_of_images_per_place]\n",
    "            selected_images = [np.asarray(Image.open(place_images[image_index]))\n",
    "                               for image_index in selected_image_indices]\n",
    "            for image in selected_images:\n",
    "                image = self._transforms(image=image)['image']\n",
    "\n",
    "                transformed_images.append(image)\n",
    "                selected_images_place_ids.append(place_index)\n",
    "\n",
    "        transformed_images, selected_images_place_ids = self._shuffle(transformed_images, selected_images_place_ids)\n",
    "\n",
    "        return torch.stack(transformed_images), torch.tensor(selected_images_place_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def _shuffle(images: list[torch.Tensor], place_ids: list[int]) -> tuple[list[torch.Tensor], list[int]]:\n",
    "        indices = torch.randperm(len(images))\n",
    "\n",
    "        return [images[index] for index in indices], [place_ids[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa1322f-299e-4922-b86b-7238e0d7f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 places_dirs: list[Path],\n",
    "                 number_of_images_per_place: int,\n",
    "                 transforms: Callable,\n",
    "                 return_indices: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self._places_images_paths: list[list[Path]] = [\n",
    "            sorted([image_path for image_path in place_dir.iterdir()\n",
    "                    if image_path.is_file()])[:number_of_images_per_place]\n",
    "            for place_dir in places_dirs\n",
    "        ]\n",
    "        self._number_of_images_per_place = number_of_images_per_place\n",
    "        self._transforms = transforms\n",
    "        self._return_indices = return_indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._places_images_paths) * self._number_of_images_per_place\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor] | tuple[torch.Tensor, torch.Tensor, int, int]:\n",
    "        place_index = index // self._number_of_images_per_place\n",
    "        image_index = index % self._number_of_images_per_place\n",
    "        image_path = self._places_images_paths[place_index][image_index]\n",
    "\n",
    "        image = np.asarray(Image.open(image_path))\n",
    "        image = self._transforms(image=image)['image']\n",
    "\n",
    "        if self._return_indices:\n",
    "            return image, torch.tensor(place_index), place_index, image_index\n",
    "\n",
    "        return image, torch.tensor(place_index)\n",
    "\n",
    "    def get_path_by_index(self, place_id: int, image_index: int) -> Path:\n",
    "        return self._places_images_paths[place_id][image_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ef57eb-e516-4b49-ad3b-f91ae5a6ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 test_dir: Path,\n",
    "                 transforms: Callable):\n",
    "        super().__init__()\n",
    "\n",
    "        self._images_paths: list[Path] = sorted([image_path for image_path in test_dir.iterdir()])\n",
    "        self._transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._images_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, str]:\n",
    "        image_path = self._images_paths[index]\n",
    "\n",
    "        image = np.asarray(Image.open(image_path))\n",
    "        image = self._transforms(image=image)['image']\n",
    "\n",
    "        return image, image_path.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bc8c9db-6b65-40b1-ab3e-572f115b0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import albumentations.pytorch\n",
    "import timm.data\n",
    "from lightning import pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MetricLearningDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path: Path, number_of_places_per_batch: int, number_of_images_per_place: int,\n",
    "                 number_of_batches_per_epoch: int, augment: bool, validation_batch_size: int, number_of_workers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self._data_path = Path(data_path)\n",
    "        self._number_of_places_per_batch = number_of_places_per_batch\n",
    "        self._number_of_images_per_place = number_of_images_per_place\n",
    "        self._number_of_batches_per_epoch = number_of_batches_per_epoch\n",
    "        self._validation_batch_size = validation_batch_size\n",
    "        self._number_of_workers = number_of_workers\n",
    "\n",
    "        self._transforms = albumentations.Compose([\n",
    "            albumentations.CenterCrop(512, 512),\n",
    "            albumentations.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD),\n",
    "            albumentations.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "        self._augmentations = albumentations.Compose([\n",
    "            albumentations.HorizontalFlip(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.Rotate(limit=15, p=1.0),\n",
    "            albumentations.Affine(scale=(0.9, 1.1), translate_percent=(-0.1, 0.1), p=1.0),\n",
    "            albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
    "            albumentations.HueSaturationValue(p=0.5),\n",
    "            albumentations.CenterCrop(512, 512),\n",
    "            albumentations.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD),\n",
    "            albumentations.CoarseDropout(max_holes=8, max_height=32, max_width=32, min_holes=1, min_height=8, min_width=8, fill_value=0),\n",
    "            albumentations.pytorch.transforms.ToTensorV2()\n",
    "        ]) if augment else self._transforms\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.easy_test_dataset = None\n",
    "        self.medium_test_dataset = None\n",
    "        self.hard_test_dataset = None\n",
    "        self.predict_dataset = None\n",
    "\n",
    "    def get_places_dirs(self, data_dir: Path) -> list[Path]:\n",
    "        return sorted(\n",
    "            [place_dir for place_dir in data_dir.iterdir()\n",
    "             if place_dir.is_dir() and len(list(place_dir.iterdir())) >= self._number_of_images_per_place]\n",
    "        )\n",
    "\n",
    "    def get_number_of_places(self, subset: str) -> int:\n",
    "        assert subset in ['train', 'val', 'test']\n",
    "        return len(self.get_places_dirs(self._data_path / subset))\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        train_places_dirs = self.get_places_dirs(self._data_path / 'train')\n",
    "        # TODO: validation dataset size can be changed\n",
    "        train_places_dirs, val_places_dirs = train_test_split(train_places_dirs, test_size=0.2, random_state=42)\n",
    "\n",
    "        print(f'Number of train places: {len(train_places_dirs)}')\n",
    "        print(f'Number of val places: {len(val_places_dirs)}')\n",
    "\n",
    "        self.train_dataset = MetricLearningDataset(\n",
    "            train_places_dirs,\n",
    "            self._number_of_places_per_batch,\n",
    "            self._number_of_images_per_place,\n",
    "            self._number_of_batches_per_epoch,\n",
    "            self._augmentations,\n",
    "        )\n",
    "        self.val_dataset = EvaluationDataset(\n",
    "            val_places_dirs,\n",
    "            self._number_of_images_per_place,\n",
    "            self._transforms,\n",
    "        )\n",
    "        self.predict_dataset = PredictionDataset(\n",
    "            self._data_path / 'test',\n",
    "            self._transforms\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=1, num_workers=self._number_of_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self._validation_batch_size, num_workers=self._number_of_workers,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.predict_dataset, batch_size=self._validation_batch_size, num_workers=self._number_of_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad0fa397-c83b-4bc5-a27b-66a483f4967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from pytorch_metric_learning.distances import BaseDistance\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from pytorch_metric_learning.utils.inference import CustomKNN\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "class MultiMetric(Metric):\n",
    "    def __init__(self, distance: BaseDistance):\n",
    "        super().__init__()\n",
    "\n",
    "        logger = logging.getLogger('PML')\n",
    "        logger.setLevel(logging.WARN)\n",
    "\n",
    "        knn = CustomKNN(distance, batch_size=256)\n",
    "        self.calculator = AccuracyCalculator(include=('precision_at_1', 'mean_average_precision'), k=4,\n",
    "                                             device=torch.device('cpu'),\n",
    "                                             knn_func=knn)\n",
    "        self.metric_names = self.calculator.get_curr_metrics()\n",
    "\n",
    "        for metric_name in self.metric_names:\n",
    "            self.add_state(metric_name, default=torch.tensor(0, dtype=torch.float32), dist_reduce_fx='sum')\n",
    "\n",
    "        self.add_state('count', default=torch.tensor(0, dtype=torch.int64), dist_reduce_fx='sum')\n",
    "\n",
    "    def update(self, vectors, labels):\n",
    "        vectors = vectors.detach().cpu() if vectors.requires_grad else vectors.cpu()\n",
    "        labels = labels.detach().cpu() if labels.requires_grad else labels.cpu()\n",
    "        results = self.calculator.get_accuracy(vectors, labels, include=('precision_at_1', 'mean_average_precision'))\n",
    "        for metric_name, metric_value in results.items():\n",
    "            metric_state = getattr(self, metric_name)\n",
    "            metric_state += metric_value\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "    def compute(self):\n",
    "        return {\n",
    "            metric_name: getattr(self, metric_name) / self.count for metric_name in self.metric_names\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4d574cb-403a-4c19-8b3d-2ec708a64cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.linalg\n",
    "from lightning import pytorch as pl\n",
    "from pytorch_metric_learning import miners, losses, distances\n",
    "from torchmetrics import MetricCollection\n",
    "\n",
    "\n",
    "class EmbeddingModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 embedding_size: int,\n",
    "                 lr: float,\n",
    "                 lr_patience: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.lr_patience = lr_patience\n",
    "\n",
    "        self.network = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=embedding_size)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        # TODO: The distance, the miner and the loss function are subject to change\n",
    "        # TODO: Adding embedding regularization is probably a good idea\n",
    "        self.distance = distances.CosineSimilarity()\n",
    "        self.miner = miners.MultiSimilarityMiner(distance=self.distance)\n",
    "        self.loss_function = losses.TripletMarginLoss(distance=self.distance)\n",
    "\n",
    "        self.val_outputs = None\n",
    "\n",
    "        metrics = MetricCollection(MultiMetric(distance=self.distance))\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.squeeze(0)\n",
    "        y = y.squeeze(0)\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_function(y_pred, y, self.miner(y_pred, y))\n",
    "        self.log('train_loss', loss, sync_dist=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "        x = x.squeeze(0)\n",
    "        y = y.squeeze(0)\n",
    "        y_pred = self.forward(x)\n",
    "        self.val_outputs['preds'].append(y_pred.cpu())\n",
    "        self.val_outputs['targets'].append(y.cpu())\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, **kwargs) -> tuple[torch.Tensor, list[str]]:\n",
    "        x, y = batch\n",
    "        x = x.squeeze(0)\n",
    "        y_pred = self.forward(x)\n",
    "        return y_pred.cpu(), y\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self.val_outputs = {\n",
    "            'preds': [],\n",
    "            'targets': [],\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        preds = torch.cat(self.val_outputs['preds'], dim=0)\n",
    "        targets = torch.cat(self.val_outputs['targets'], dim=0)\n",
    "        self.log_dict(self.val_metrics(preds, targets), sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.lr_patience)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_precision_at_1',\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "af4b86dc-8494-4fe5-ba6c-6e475f02e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/aadamczak/ZPO/e/ZPO-87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "    | Name                          | Type                 | Params\n",
      "-------------------------------------------------------------------------\n",
      "0   | network                       | ResNet               | 21.8 M\n",
      "1   | network.conv1                 | Conv2d               | 9.4 K \n",
      "2   | network.bn1                   | BatchNorm2d          | 128   \n",
      "3   | network.act1                  | ReLU                 | 0     \n",
      "4   | network.maxpool               | MaxPool2d            | 0     \n",
      "5   | network.layer1                | Sequential           | 221 K \n",
      "6   | network.layer1.0              | BasicBlock           | 74.0 K\n",
      "7   | network.layer1.0.conv1        | Conv2d               | 36.9 K\n",
      "8   | network.layer1.0.bn1          | BatchNorm2d          | 128   \n",
      "9   | network.layer1.0.drop_block   | Identity             | 0     \n",
      "10  | network.layer1.0.act1         | ReLU                 | 0     \n",
      "11  | network.layer1.0.aa           | Identity             | 0     \n",
      "12  | network.layer1.0.conv2        | Conv2d               | 36.9 K\n",
      "13  | network.layer1.0.bn2          | BatchNorm2d          | 128   \n",
      "14  | network.layer1.0.act2         | ReLU                 | 0     \n",
      "15  | network.layer1.1              | BasicBlock           | 74.0 K\n",
      "16  | network.layer1.1.conv1        | Conv2d               | 36.9 K\n",
      "17  | network.layer1.1.bn1          | BatchNorm2d          | 128   \n",
      "18  | network.layer1.1.drop_block   | Identity             | 0     \n",
      "19  | network.layer1.1.act1         | ReLU                 | 0     \n",
      "20  | network.layer1.1.aa           | Identity             | 0     \n",
      "21  | network.layer1.1.conv2        | Conv2d               | 36.9 K\n",
      "22  | network.layer1.1.bn2          | BatchNorm2d          | 128   \n",
      "23  | network.layer1.1.act2         | ReLU                 | 0     \n",
      "24  | network.layer1.2              | BasicBlock           | 74.0 K\n",
      "25  | network.layer1.2.conv1        | Conv2d               | 36.9 K\n",
      "26  | network.layer1.2.bn1          | BatchNorm2d          | 128   \n",
      "27  | network.layer1.2.drop_block   | Identity             | 0     \n",
      "28  | network.layer1.2.act1         | ReLU                 | 0     \n",
      "29  | network.layer1.2.aa           | Identity             | 0     \n",
      "30  | network.layer1.2.conv2        | Conv2d               | 36.9 K\n",
      "31  | network.layer1.2.bn2          | BatchNorm2d          | 128   \n",
      "32  | network.layer1.2.act2         | ReLU                 | 0     \n",
      "33  | network.layer2                | Sequential           | 1.1 M \n",
      "34  | network.layer2.0              | BasicBlock           | 230 K \n",
      "35  | network.layer2.0.conv1        | Conv2d               | 73.7 K\n",
      "36  | network.layer2.0.bn1          | BatchNorm2d          | 256   \n",
      "37  | network.layer2.0.drop_block   | Identity             | 0     \n",
      "38  | network.layer2.0.act1         | ReLU                 | 0     \n",
      "39  | network.layer2.0.aa           | Identity             | 0     \n",
      "40  | network.layer2.0.conv2        | Conv2d               | 147 K \n",
      "41  | network.layer2.0.bn2          | BatchNorm2d          | 256   \n",
      "42  | network.layer2.0.act2         | ReLU                 | 0     \n",
      "43  | network.layer2.0.downsample   | Sequential           | 8.4 K \n",
      "44  | network.layer2.0.downsample.0 | Conv2d               | 8.2 K \n",
      "45  | network.layer2.0.downsample.1 | BatchNorm2d          | 256   \n",
      "46  | network.layer2.1              | BasicBlock           | 295 K \n",
      "47  | network.layer2.1.conv1        | Conv2d               | 147 K \n",
      "48  | network.layer2.1.bn1          | BatchNorm2d          | 256   \n",
      "49  | network.layer2.1.drop_block   | Identity             | 0     \n",
      "50  | network.layer2.1.act1         | ReLU                 | 0     \n",
      "51  | network.layer2.1.aa           | Identity             | 0     \n",
      "52  | network.layer2.1.conv2        | Conv2d               | 147 K \n",
      "53  | network.layer2.1.bn2          | BatchNorm2d          | 256   \n",
      "54  | network.layer2.1.act2         | ReLU                 | 0     \n",
      "55  | network.layer2.2              | BasicBlock           | 295 K \n",
      "56  | network.layer2.2.conv1        | Conv2d               | 147 K \n",
      "57  | network.layer2.2.bn1          | BatchNorm2d          | 256   \n",
      "58  | network.layer2.2.drop_block   | Identity             | 0     \n",
      "59  | network.layer2.2.act1         | ReLU                 | 0     \n",
      "60  | network.layer2.2.aa           | Identity             | 0     \n",
      "61  | network.layer2.2.conv2        | Conv2d               | 147 K \n",
      "62  | network.layer2.2.bn2          | BatchNorm2d          | 256   \n",
      "63  | network.layer2.2.act2         | ReLU                 | 0     \n",
      "64  | network.layer2.3              | BasicBlock           | 295 K \n",
      "65  | network.layer2.3.conv1        | Conv2d               | 147 K \n",
      "66  | network.layer2.3.bn1          | BatchNorm2d          | 256   \n",
      "67  | network.layer2.3.drop_block   | Identity             | 0     \n",
      "68  | network.layer2.3.act1         | ReLU                 | 0     \n",
      "69  | network.layer2.3.aa           | Identity             | 0     \n",
      "70  | network.layer2.3.conv2        | Conv2d               | 147 K \n",
      "71  | network.layer2.3.bn2          | BatchNorm2d          | 256   \n",
      "72  | network.layer2.3.act2         | ReLU                 | 0     \n",
      "73  | network.layer3                | Sequential           | 6.8 M \n",
      "74  | network.layer3.0              | BasicBlock           | 919 K \n",
      "75  | network.layer3.0.conv1        | Conv2d               | 294 K \n",
      "76  | network.layer3.0.bn1          | BatchNorm2d          | 512   \n",
      "77  | network.layer3.0.drop_block   | Identity             | 0     \n",
      "78  | network.layer3.0.act1         | ReLU                 | 0     \n",
      "79  | network.layer3.0.aa           | Identity             | 0     \n",
      "80  | network.layer3.0.conv2        | Conv2d               | 589 K \n",
      "81  | network.layer3.0.bn2          | BatchNorm2d          | 512   \n",
      "82  | network.layer3.0.act2         | ReLU                 | 0     \n",
      "83  | network.layer3.0.downsample   | Sequential           | 33.3 K\n",
      "84  | network.layer3.0.downsample.0 | Conv2d               | 32.8 K\n",
      "85  | network.layer3.0.downsample.1 | BatchNorm2d          | 512   \n",
      "86  | network.layer3.1              | BasicBlock           | 1.2 M \n",
      "87  | network.layer3.1.conv1        | Conv2d               | 589 K \n",
      "88  | network.layer3.1.bn1          | BatchNorm2d          | 512   \n",
      "89  | network.layer3.1.drop_block   | Identity             | 0     \n",
      "90  | network.layer3.1.act1         | ReLU                 | 0     \n",
      "91  | network.layer3.1.aa           | Identity             | 0     \n",
      "92  | network.layer3.1.conv2        | Conv2d               | 589 K \n",
      "93  | network.layer3.1.bn2          | BatchNorm2d          | 512   \n",
      "94  | network.layer3.1.act2         | ReLU                 | 0     \n",
      "95  | network.layer3.2              | BasicBlock           | 1.2 M \n",
      "96  | network.layer3.2.conv1        | Conv2d               | 589 K \n",
      "97  | network.layer3.2.bn1          | BatchNorm2d          | 512   \n",
      "98  | network.layer3.2.drop_block   | Identity             | 0     \n",
      "99  | network.layer3.2.act1         | ReLU                 | 0     \n",
      "100 | network.layer3.2.aa           | Identity             | 0     \n",
      "101 | network.layer3.2.conv2        | Conv2d               | 589 K \n",
      "102 | network.layer3.2.bn2          | BatchNorm2d          | 512   \n",
      "103 | network.layer3.2.act2         | ReLU                 | 0     \n",
      "104 | network.layer3.3              | BasicBlock           | 1.2 M \n",
      "105 | network.layer3.3.conv1        | Conv2d               | 589 K \n",
      "106 | network.layer3.3.bn1          | BatchNorm2d          | 512   \n",
      "107 | network.layer3.3.drop_block   | Identity             | 0     \n",
      "108 | network.layer3.3.act1         | ReLU                 | 0     \n",
      "109 | network.layer3.3.aa           | Identity             | 0     \n",
      "110 | network.layer3.3.conv2        | Conv2d               | 589 K \n",
      "111 | network.layer3.3.bn2          | BatchNorm2d          | 512   \n",
      "112 | network.layer3.3.act2         | ReLU                 | 0     \n",
      "113 | network.layer3.4              | BasicBlock           | 1.2 M \n",
      "114 | network.layer3.4.conv1        | Conv2d               | 589 K \n",
      "115 | network.layer3.4.bn1          | BatchNorm2d          | 512   \n",
      "116 | network.layer3.4.drop_block   | Identity             | 0     \n",
      "117 | network.layer3.4.act1         | ReLU                 | 0     \n",
      "118 | network.layer3.4.aa           | Identity             | 0     \n",
      "119 | network.layer3.4.conv2        | Conv2d               | 589 K \n",
      "120 | network.layer3.4.bn2          | BatchNorm2d          | 512   \n",
      "121 | network.layer3.4.act2         | ReLU                 | 0     \n",
      "122 | network.layer3.5              | BasicBlock           | 1.2 M \n",
      "123 | network.layer3.5.conv1        | Conv2d               | 589 K \n",
      "124 | network.layer3.5.bn1          | BatchNorm2d          | 512   \n",
      "125 | network.layer3.5.drop_block   | Identity             | 0     \n",
      "126 | network.layer3.5.act1         | ReLU                 | 0     \n",
      "127 | network.layer3.5.aa           | Identity             | 0     \n",
      "128 | network.layer3.5.conv2        | Conv2d               | 589 K \n",
      "129 | network.layer3.5.bn2          | BatchNorm2d          | 512   \n",
      "130 | network.layer3.5.act2         | ReLU                 | 0     \n",
      "131 | network.layer4                | Sequential           | 13.1 M\n",
      "132 | network.layer4.0              | BasicBlock           | 3.7 M \n",
      "133 | network.layer4.0.conv1        | Conv2d               | 1.2 M \n",
      "134 | network.layer4.0.bn1          | BatchNorm2d          | 1.0 K \n",
      "135 | network.layer4.0.drop_block   | Identity             | 0     \n",
      "136 | network.layer4.0.act1         | ReLU                 | 0     \n",
      "137 | network.layer4.0.aa           | Identity             | 0     \n",
      "138 | network.layer4.0.conv2        | Conv2d               | 2.4 M \n",
      "139 | network.layer4.0.bn2          | BatchNorm2d          | 1.0 K \n",
      "140 | network.layer4.0.act2         | ReLU                 | 0     \n",
      "141 | network.layer4.0.downsample   | Sequential           | 132 K \n",
      "142 | network.layer4.0.downsample.0 | Conv2d               | 131 K \n",
      "143 | network.layer4.0.downsample.1 | BatchNorm2d          | 1.0 K \n",
      "144 | network.layer4.1              | BasicBlock           | 4.7 M \n",
      "145 | network.layer4.1.conv1        | Conv2d               | 2.4 M \n",
      "146 | network.layer4.1.bn1          | BatchNorm2d          | 1.0 K \n",
      "147 | network.layer4.1.drop_block   | Identity             | 0     \n",
      "148 | network.layer4.1.act1         | ReLU                 | 0     \n",
      "149 | network.layer4.1.aa           | Identity             | 0     \n",
      "150 | network.layer4.1.conv2        | Conv2d               | 2.4 M \n",
      "151 | network.layer4.1.bn2          | BatchNorm2d          | 1.0 K \n",
      "152 | network.layer4.1.act2         | ReLU                 | 0     \n",
      "153 | network.layer4.2              | BasicBlock           | 4.7 M \n",
      "154 | network.layer4.2.conv1        | Conv2d               | 2.4 M \n",
      "155 | network.layer4.2.bn1          | BatchNorm2d          | 1.0 K \n",
      "156 | network.layer4.2.drop_block   | Identity             | 0     \n",
      "157 | network.layer4.2.act1         | ReLU                 | 0     \n",
      "158 | network.layer4.2.aa           | Identity             | 0     \n",
      "159 | network.layer4.2.conv2        | Conv2d               | 2.4 M \n",
      "160 | network.layer4.2.bn2          | BatchNorm2d          | 1.0 K \n",
      "161 | network.layer4.2.act2         | ReLU                 | 0     \n",
      "162 | network.global_pool           | SelectAdaptivePool2d | 0     \n",
      "163 | network.global_pool.pool      | AdaptiveAvgPool2d    | 0     \n",
      "164 | network.global_pool.flatten   | Flatten              | 0     \n",
      "165 | network.fc                    | Linear               | 525 K \n",
      "166 | dropout                       | Dropout              | 0     \n",
      "167 | distance                      | CosineSimilarity     | 0     \n",
      "168 | miner                         | MultiSimilarityMiner | 0     \n",
      "169 | loss_function                 | TripletMarginLoss    | 0     \n",
      "170 | loss_function.reducer         | AvgNonZeroReducer    | 0     \n",
      "171 | val_metrics                   | MetricCollection     | 0     \n",
      "172 | val_metrics.MultiMetric       | MultiMetric          | 0     \n",
      "-------------------------------------------------------------------------\n",
      "21.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.8 M    Total params\n",
      "87.240    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train places: 800\n",
      "Number of val places: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf186e2e3aae45bea35d1de50e3206cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'val_precision_at_1' reached 0.77700 (best 0.77700), saving model to '/.neptune/Untitled/ZPO-87/checkpoints/epoch=0-val_precision_at_1=0.77700.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 200: 'val_precision_at_1' reached 0.82200 (best 0.82200), saving model to '/.neptune/Untitled/ZPO-87/checkpoints/epoch=1-val_precision_at_1=0.82200.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 300: 'val_precision_at_1' reached 0.87600 (best 0.87600), saving model to '/.neptune/Untitled/ZPO-87/checkpoints/epoch=2-val_precision_at_1=0.87600.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 400: 'val_precision_at_1' reached 0.89500 (best 0.89500), saving model to '/.neptune/Untitled/ZPO-87/checkpoints/epoch=3-val_precision_at_1=0.89500.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 500: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 600: 'val_precision_at_1' reached 0.90500 (best 0.90500), saving model to '/.neptune/Untitled/ZPO-87/checkpoints/epoch=5-val_precision_at_1=0.90500.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 700: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 800: 'val_precision_at_1' reached 0.91500 (best 0.91500), saving model to '/.neptune/Untitled/ZPO-87/checkpoints/epoch=7-val_precision_at_1=0.91500.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 900: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1000: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1100: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 1200: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 1300: 'val_precision_at_1' reached 0.91700 (best 0.91700), saving model to '/.neptune/Untitled/ZPO-87/checkpoints/epoch=12-val_precision_at_1=0.91700.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 1400: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 1500: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 1600: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 1700: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 1800: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 1900: 'val_precision_at_1' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 2000: 'val_precision_at_1' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# TODO: experiment with data module and model settings\n",
    "datamodule = MetricLearningDataModule(\n",
    "    data_path=Path('.'),\n",
    "    number_of_places_per_batch=16,\n",
    "    number_of_images_per_place=5,\n",
    "    number_of_batches_per_epoch=100,\n",
    "    augment=True,\n",
    "    validation_batch_size=24,\n",
    "    number_of_workers=6\n",
    ")\n",
    "model = EmbeddingModel(\n",
    "    embedding_size=1024,\n",
    "    lr=1e-3,\n",
    "    lr_patience=10\n",
    ")\n",
    "\n",
    "model_summary_callback = pl.callbacks.ModelSummary(max_depth=-1)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(filename='{epoch}-{val_precision_at_1:.5f}', mode='max',\n",
    "                                                    monitor='val_precision_at_1', verbose=True, save_last=True)\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(monitor='val_precision_at_1', mode='max', patience=50)\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "logger = pl.loggers.NeptuneLogger(project=\"aadamczak/ZPO\",\n",
    "    api_token=\"\") #Neptune api\n",
    "trainer = pl.Trainer(logger=logger,\n",
    "    callbacks=[model_summary_callback, checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "    accelerator='gpu',\n",
    "    max_epochs=50\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38f0c52d-cbe0-481f-8309-43c7f4e3163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /.neptune/Untitled/ZPO-86/checkpoints/epoch=5-val_precision_at_1=0.83625.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train places: 800\n",
      "Number of val places: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /.neptune/Untitled/ZPO-86/checkpoints/epoch=5-val_precision_at_1=0.83625.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098f0891dcd946d1b928bf9ecf2d5de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(model=model, ckpt_path=checkpoint_callback.best_model_path, datamodule=datamodule)\n",
    "\n",
    "results = {}\n",
    "for prediction in predictions:\n",
    "    for embedding, identifier in zip(*prediction):\n",
    "        results[identifier] = embedding.tolist()\n",
    "\n",
    "with open('results.pickle', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40995bf7-7c29-44eb-a215-1163f6229163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rank_1_accuracy': 0.5724}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def main():\n",
    "    student_id = 144610 # TODO: put your student id here\n",
    "    distance_name = 'cosine'  # supported values are: manhattan, euclidean, cosine\n",
    "    with open('results.pickle', 'rb') as file:\n",
    "        predictions = file.read()\n",
    "\n",
    "    response = requests.post(f'https://zpo.dpieczynski.pl/{student_id}',\n",
    "                             headers={'distance': distance_name},\n",
    "                             data=predictions)\n",
    "    print(response.json())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201e0f8-e124-4a72-bd17-efe875a0c644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e17f2e-155e-489b-abcc-e110758e6b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb0ebb-87e5-4d61-8fb5-4fc4b7e4d6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
